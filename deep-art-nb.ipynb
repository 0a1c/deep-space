{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(713, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train = None\n",
    "\n",
    "def load_data():\n",
    "    global X_train\n",
    "    \n",
    "    X_train = np.loadtxt(\"Data.txt\")\n",
    "    X_train_temp = X_train.reshape(-1, 3)\n",
    "    X_train_mean = np.mean(X_train_temp, axis=0)\n",
    "    X_train_std = np.std(X_train_temp, axis=0)\n",
    "    X_train = (X_train_temp - X_train_mean) / X_train_std\n",
    "    #X_train.resize(713, 100, 100, 3)\n",
    "    X_train.resize(713, 28, 28, 3)\n",
    "\n",
    "    \n",
    "load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_noise(batch_size, dim):\n",
    "    \"\"\"\n",
    "    Generate a PyTorch Tensor of uniform random noise.\n",
    "\n",
    "    Input:\n",
    "    - batch_size: Integer giving the batch size of noise to generate.\n",
    "    - dim: Integer giving the dimension of noise to generate.\n",
    "    \n",
    "    Output:\n",
    "    - A PyTorch Tensor of shape (batch_size, dim) containing uniform\n",
    "      random noise in the range (-1, 1).\n",
    "    \"\"\"\n",
    "    rand = torch.rand([batch_size, dim]) * 2 - 1\n",
    "    return rand\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size() # read in N, C, H, W\n",
    "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "    \n",
    "class Unflatten(nn.Module):\n",
    "    \"\"\"\n",
    "    An Unflatten module receives an input of shape (N, C*H*W) and reshapes it\n",
    "    to produce an output of shape (N, C, H, W).\n",
    "    \"\"\"\n",
    "    def __init__(self, N=-1, C=128, H=7, W=7):\n",
    "        super(Unflatten, self).__init__()\n",
    "        self.N = N\n",
    "        self.C = C\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "    def forward(self, x):\n",
    "        return x.view(self.N, self.C, self.H, self.W)\n",
    "    \n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.ConvTranspose2d):\n",
    "        init.xavier_uniform(m.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MISC SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "NOISE_DIM = 96\n",
    "X_train = torch.from_numpy(X_train)\n",
    "X_train = torch.transpose(X_train, 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU OR GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dtype = torch.FloatTensor\n",
    "dtype = torch.cuda.FloatTensor ## UNCOMMENT THIS LINE IF YOU'RE ON A GPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ls_discriminator_loss(scores_real, scores_fake):\n",
    "    \"\"\"\n",
    "    Compute the Least-Squares GAN loss for the discriminator.\n",
    "    \n",
    "    Inputs:\n",
    "    - scores_real: PyTorch Variable of shape (N,) giving scores for the real data.\n",
    "    - scores_fake: PyTorch Variable of shape (N,) giving scores for the fake data.\n",
    "    \n",
    "    Outputs:\n",
    "    - loss: A PyTorch Variable containing the loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    sq_real_loss = (scores_real - 1) ** 2\n",
    "    sq_fake_loss = (scores_fake ** 2)\n",
    "    \n",
    "    \n",
    "    loss = sq_real_loss.mean() / 2 + sq_fake_loss.mean() / 2\n",
    "    return loss\n",
    "\n",
    "def ls_generator_loss(scores_fake):\n",
    "    \"\"\"\n",
    "    Computes the Least-Squares GAN loss for the generator.\n",
    "    \n",
    "    Inputs:\n",
    "    - scores_fake: PyTorch Variable of shape (N,) giving scores for the fake data.\n",
    "    \n",
    "    Outputs:\n",
    "    - loss: A PyTorch Variable containing the loss.\n",
    "    \"\"\"\n",
    "    sq_loss = (scores_fake - 1) ** 2\n",
    "    loss = sq_loss.mean() / 2\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_optimizer(model):\n",
    "    \"\"\"\n",
    "    Construct and return an Adam optimizer for the model with learning rate 1e-3,\n",
    "    beta1=0.5, and beta2=0.999.\n",
    "    \n",
    "    Input:\n",
    "    - model: A PyTorch model that we want to optimize.\n",
    "    \n",
    "    Returns:\n",
    "    - An Adam optimizer for the model with the desired hyperparameters.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_a_gan(D, G, D_solver, G_solver, discriminator_loss, generator_loss, show_every=250, \n",
    "              batch_size=128, noise_size=96, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Train a GAN!\n",
    "    \n",
    "    Inputs:\n",
    "    - D, G: PyTorch models for the discriminator and generator\n",
    "    - D_solver, G_solver: torch.optim Optimizers to use for training the\n",
    "      discriminator and generator.\n",
    "    - discriminator_loss, generator_loss: Functions to use for computing the generator and\n",
    "      discriminator loss, respectively.\n",
    "    - show_every: Show samples after every show_every iterations.\n",
    "    - batch_size: Batch size to use for training.\n",
    "    - noise_size: Dimension of the noise to use as input to the generator.\n",
    "    - num_epochs: Number of epochs over the training dataset to use for training.\n",
    "    \"\"\"\n",
    "    iter_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for x, _ in loader_train:\n",
    "            if len(x) != batch_size:\n",
    "                continue\n",
    "            D_solver.zero_grad()\n",
    "            real_data = Variable(x).type(dtype)\n",
    "            logits_real = D(2* (real_data - 0.5)).type(dtype)\n",
    "\n",
    "            g_fake_seed = Variable(sample_noise(batch_size, noise_size)).type(dtype)\n",
    "            fake_images = G(g_fake_seed).detach()\n",
    "            logits_fake = D(fake_images.view(batch_size, 1, 28, 28))\n",
    "\n",
    "            d_total_error = discriminator_loss(logits_real, logits_fake)\n",
    "            d_total_error.backward()        \n",
    "            D_solver.step()\n",
    "\n",
    "            G_solver.zero_grad()\n",
    "            g_fake_seed = Variable(sample_noise(batch_size, noise_size)).type(dtype)\n",
    "            fake_images = G(g_fake_seed)\n",
    "\n",
    "            gen_logits_fake = D(fake_images.view(batch_size, 1, 28, 28))\n",
    "            g_error = generator_loss(gen_logits_fake)\n",
    "            g_error.backward()\n",
    "            G_solver.step()\n",
    "\n",
    "            if (iter_count % show_every == 0):\n",
    "                print('Iter: {}, D: {:.4}, G:{:.4}'.format(iter_count,d_total_error.data[0],g_error.data[0]))\n",
    "                imgs_numpy = fake_images.data.cpu().numpy()\n",
    "                show_images(imgs_numpy[0:16])\n",
    "                plt.show()\n",
    "                print()\n",
    "            iter_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DISCRIMINATOR AND GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([713, 3, 100, 100])\n",
      "torch.Size([713, 1])\n"
     ]
    }
   ],
   "source": [
    "def build_dc_classifier():\n",
    "    \"\"\"\n",
    "    Build and return a PyTorch model for the DCGAN discriminator implementing\n",
    "    the architecture above.\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        ###########################\n",
    "        ######### TO DO ###########\n",
    "        ###########################\n",
    "        Unflatten(batch_size, 3, 28, 28),\n",
    "#        Unflatten(batch_size, 3, 100, 100),\n",
    "        nn.Conv2d(3, 32, 5),\n",
    "        nn.LeakyReLU(0.01, inplace=True),\n",
    "        nn.MaxPool2d(2, stride=2),\n",
    "        nn.Conv2d(32, 64, 5),\n",
    "        nn.LeakyReLU(0.01, inplace=True),\n",
    "        nn.MaxPool2d(2, stride=2),\n",
    "        Flatten(),\n",
    "        nn.Linear(4*4*64, 4*4*64),\n",
    "        nn.Linear(4*4*64, 1)\n",
    "#         nn.Linear(22*22*64, 22*22*64),\n",
    "#         nn.Linear(22*22*64, 1)\n",
    "    )\n",
    "\n",
    "data = Variable(X_train).type(dtype)\n",
    "print(data.size())\n",
    "b = build_dc_classifier().type(dtype)\n",
    "out = b(data)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([713, 784])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_dc_generator(noise_dim=NOISE_DIM):\n",
    "    \"\"\"\n",
    "    Build and return a PyTorch model implementing the DCGAN generator using\n",
    "    the architecture described above.\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        #Flatten(),\n",
    "        nn.Linear(noise_dim, 1024),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm1d(1024),\n",
    "        nn.Linear(1024, 7*7*128),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm1d(7*7*128),\n",
    "        Unflatten(batch_size, 128, 7, 7),\n",
    "        nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ConvTranspose2d(64, 1, 4, stride=2, padding=1),\n",
    "        nn.Tanh(),\n",
    "        Flatten()\n",
    "    )\n",
    "\n",
    "test_g_gan = build_dc_generator().type(dtype)\n",
    "test_g_gan.apply(initialize_weights)\n",
    "\n",
    "fake_seed = Variable(torch.randn(batch_size, NOISE_DIM)).type(dtype)\n",
    "fake_images = test_g_gan.forward(fake_seed)\n",
    "fake_images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_DC = build_dc_classifier().type(dtype) \n",
    "D_DC.apply(initialize_weights)\n",
    "G_DC = build_dc_generator().type(dtype)\n",
    "G_DC.apply(initialize_weights)\n",
    "\n",
    "D_DC_solver = get_optimizer(D_DC)\n",
    "G_DC_solver = get_optimizer(G_DC)\n",
    "\n",
    "run_a_gan(D_DC, G_DC, D_DC_solver, G_DC_solver, discriminator_loss, generator_loss, num_epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
